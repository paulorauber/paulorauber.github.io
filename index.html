<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="Paulo Rauber.">
  <title>Paulo Rauber</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <a href="#main" class="skip-link">Skip to main content</a>

  <header>
    <div class="header-text">
      <h1>Paulo Rauber</h1>
      <p class="contact">
        Lecturer in Artificial Intelligence<br>
        Queen Mary University of London<br>
        <a href="mailto:p.rauber@qmul.ac.uk">p.rauber@qmul.ac.uk</a>
      </p>
      <nav>
        <a href="index.html" aria-current="page">About</a>
        <a href="work.html">Work</a>
      </nav>
    </div>
    <img src="files/images/profile.jpg" alt="Paulo Rauber" class="profile-pic" width="150" height="150">
  </header>

  <main id="main">
    <h2>About me</h2>
    <p>I am a lecturer in Artificial Intelligence at Queen Mary University of London. Before becoming a lecturer, I was a postdoctoral researcher in the Swiss AI lab working on reinforcement learning under the supervision of <a href="https://people.idsia.ch/~juergen/" rel="noopener noreferrer">JÃ¼rgen Schmidhuber</a>.</p>
    <p>I believe that intelligence should be defined as a measure of the ability of an agent to achieve goals in a wide range of environments, which makes reinforcement learning an excellent framework to study many challenges that intelligent agents are bound to face.</p>
    <p>My current research is focused on developing principled but scalable Bayesian reinforcement learning methods that address the most significant of these challenges: exploration, planning, and generalization.</p>
    <p>More information about my work is available <a href="work.html">here</a>.</p>

    <h2>Selected publications</h2>
    <ul>
      <li>R. Sasso, M. Conserva, P. Rauber. <em>"Posterior Sampling for Deep Reinforcement Learning"</em>, <span class="highlight">International Conference on Machine Learning (ICML)</span>, 2023.</li>
      <li>M. Conserva, P. Rauber. <em>"Hardness in Markov Decision Processes: Theory and Practice"</em>, <span class="highlight">Conference on Neural Information Processing Systems (NeurIPS)</span>, 2022.</li>
      <li>P. Rauber*, A. Ramesh*, M. Conserva, J. Schmidhuber. <em>"Recurrent Neural-Linear Posterior Sampling for Non-Stationary Contextual Bandits"</em>, <span class="highlight">Neural Computation</span>, 2022.</li>
      <li>P. Rauber, A. Ummadisingu, F. Mutz, J. Schmidhuber. <em>"Hindsight Policy Gradients"</em>, <span class="highlight">International Conference on Learning Representations (ICLR)</span>, 2019.</li>
    </ul>

    <h2>PhD students</h2>
    <ul>
      <li><a href="https://michelangeloconserva.github.io/" rel="noopener noreferrer">Michelangelo Conserva</a></li>
      <li><a href="https://remosasso.github.io/" rel="noopener noreferrer">Remo Sasso</a></li>
      <li>Connor Watts</li>
    </ul>
    <p>If you would like to pursue a PhD under my supervision, please send me a message with your curriculum vitae and a brief description of your research goals after reading <a href="recommended_reading.html">this</a>.</p>
  </main>
</body>
</html>
